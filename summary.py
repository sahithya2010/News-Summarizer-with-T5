# -*- coding: utf-8 -*-
"""final_summary_works.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cRB-VB0J9resxFq9p-xArNN2d1nMMxk2
"""

! pip install datasets transformers sentencepiece

!pip install accelerate -U

!pip install evaluate

from huggingface_hub import notebook_login

notebook_login()

!apt install git-lfs

!pip install rouge_score

!pip install nlp

import torch
import nlp
from transformers import T5Tokenizer
import nltk

model_checkpoint = "t5-small"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

from datasets import load_dataset
from evaluate import load


data=pd.read_csv("/content/techcrunch_news_data_2022_07_01.csv")
data


#raw_datasets = load_dataset(data)
#metric = load("rouge")

data.dropna()

from datasets import Dataset, DatasetDict

data_dictionary = DatasetDict({
    "total": Dataset.from_pandas(data)
    })

data_dictionary

import pandas as pd
from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(data, test_size=0.25, random_state=42)

train_data = pd.DataFrame(X_train, columns=data.columns)
test_data = pd.DataFrame(X_test, columns=data.columns)

train_data.to_csv('train.csv', index=False)
test_data.to_csv('test.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split

X_train, X_val = train_test_split(train_data, test_size=0.25, random_state=42)

train_data = pd.DataFrame(X_train, columns=data.columns)
validation_data = pd.DataFrame(X_val, columns=data.columns)

train_data.to_csv('train.csv', index=False)
validation_data.to_csv('validation.csv', index=False)

raw_datasets = DatasetDict({
    "train": Dataset.from_pandas(train_data),
    "validation": Dataset.from_pandas(validation_data),
    "test": Dataset.from_pandas(test_data)
    })

raw_datasets

train_df=pd.read_csv("/content/train.csv")
test_df=pd.read_csv("/content/test.csv")
validation_df=pd.read_csv("/content/train.csv")

import datasets
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=5):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, datasets.ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))

show_random_elements(raw_datasets["train"])

len("5733b0fb4776f41900661041")

import sklearn.metrics as metrics
import string

metrics

import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

tokenizer("Hello, this one sentence!")

raw_datasets_cleaned = raw_datasets.filter(
    lambda example: (len(example['description']) >= 500) and
    (len(example['headline']) >= 20)
)

if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:
    prefix = "summarize: "
else:
    prefix = ""

model_checkpoint

max_input_length = 512
max_target_length = 64

def clean_text(text):
  sentences = nltk.sent_tokenize(text.strip())
  sentences_cleaned = [s for sent in sentences for s in sent.split("\n")]
  sentences_cleaned_no_titles = [sent for sent in sentences_cleaned
                                 if len(sent) > 0 and
                                 sent[-1] in string.punctuation]
  text_cleaned = "\n".join(sentences_cleaned_no_titles)
  return text_cleaned

def preprocess_data(examples):
  texts_cleaned = [clean_text(text) for text in examples["description"]]
  inputs = [prefix + text for text in texts_cleaned]
  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

  # Setup the tokenizer for targets
  with tokenizer.as_target_tokenizer():
    labels = tokenizer(examples["headline"], max_length=max_target_length,
                       truncation=True)

  model_inputs["labels"] = labels["input_ids"]
  return model_inputs

raw_datasets["train"][0]

import nltk
nltk.download('punkt')

tokenized_datasets = raw_datasets_cleaned.map(preprocess_data,batched=True)



"""QUESTION ANSWERING"""

# process the examples in input and target text format and the eos token at the end
def add_eos_to_examples(examples):
    examples['input_text'] = 'question: %s  context: %s </s>' % (examples(train_df['short_description']), examples(train_df['headline']))
    examples['target_text'] = '%s </s>' % examples(train_df['uniq_id'])(train_df['raw_description'])[0]
    input_encodings = tokenizer.batch_encode_plus(examples['input_text'], pad_to_max_length=True, max_length=128)
    target_encodings = tokenizer.batch_encode_plus(examples['target_text'], pad_to_max_length=True, max_length=64)
    encodings = {
        'input_ids': input_encodings['input_ids'],
        'attention_mask': input_encodings['attention_mask'],
        'target_ids': target_encodings['input_ids'],
        'target_attention_mask': target_encodings['attention_mask']
    }

    return examples
    return encodings

add_eos_to_examples(data[:2])

tokenized_datasets_qa_train = raw_datasets.map(add_eos_to_examples, batched=True)
tokenized_datasets_qa_validation = raw_datasets.map(add_eos_to_examples, batched=True)
# set the tensor type and the columns which the dataset should return
columns = ['input_ids', 'target_ids', 'attention_mask', 'target_attention_mask']
tokenized_datasets_qa_train.set_format(type='torch', columns=columns)
tokenized_datasets_qa_validation.set_format(type='torch', columns=columns)

len(tokenized_datasets_qa_train), len(tokenized_datasets_qa_test)

# cach the dataset, so we can load it directly for training

torch.save(tokenized_datasets_qa_train, 'train.pt')
torch.save(tokenized_datasets_qa_validation, 'validation.pt')

from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

batch_size = 4
model_name="final"
output_dir="final"
args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    evaluation_strategy="steps",
    eval_steps=100,
    logging_strategy="steps",
    logging_steps=100,
    save_strategy="steps",
    save_steps=200,
    learning_rate=4e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    predict_with_generate=True,
    load_best_model_at_end=True,
    metric_for_best_model="rouge1",
    push_to_hub=True
)

data_collator = DataCollatorForSeq2Seq(tokenizer)

from datasets import load_metric

import numpy as np

metric = load_metric("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Rouge expects a newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip()))
                      for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip()))
                      for label in decoded_labels]

    # Compute ROUGE scores
    result = metric.compute(predictions=decoded_preds, references=decoded_labels,
                            use_stemmer=True)

    # Extract ROUGE f1 scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}

    # Add mean generated length to metrics
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)
                      for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    return {k: round(v, 4) for k, v in result.items()}

import torch

# get test split
test_tokenized_dataset = tokenized_datasets["test"]

# pad texts to the same length
def preprocess_test(examples):
  inputs = [prefix + text for text in examples["description"]]
  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,
                           padding="max_length")
  return model_inputs

test_tokenized_dataset = test_tokenized_dataset.map(preprocess_test, batched=True)

# prepare dataloader
test_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])
dataloader = torch.utils.data.DataLoader(test_tokenized_dataset, batch_size=32)

# generate text for each batch
all_predictions = []
for i,batch in enumerate(dataloader):
  predictions = model.generate(**batch)
  all_predictions.append(predictions)

# flatten predictions
all_predictions_flattened = [pred for preds in all_predictions for pred in preds]

# tokenize and pad titles
all_titles = tokenizer(test_tokenized_dataset["headline"], max_length=max_target_length,
                       truncation=True, padding="max_length")["input_ids"]

# compute metrics
predictions_labels = [all_predictions_flattened, all_titles]
compute_metrics(predictions_labels)

# Function that returns an untrained model to be trained
def model_init():
    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

trainer = Seq2SeqTrainer(
    model_init=model_init,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.save_model()

from huggingface_hub import notebook_login

notebook_login()

from transformers import T5ForConditionalGeneration, TFT5ForConditionalGeneration, FlaxT5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained(output_dir)
pt_model = T5ForConditionalGeneration.from_pretrained(output_dir)

#flax_model = FlaxT5ForConditionalGeneration.from_pretrained(output_dir, from_pt=True)

tokenizer.push_to_hub(model_name)
pt_model.push_to_hub(model_name)

#flax_model.push_to_hub(model_name)

!pip install transformers



from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import nltk
nltk.download('punkt')

tokenizer = AutoTokenizer.from_pretrained("sahithya20/final")
model = AutoModelForSeq2SeqLM.from_pretrained("sahithya20/final")

text = """Many financial institutions started building conversational AI, prior to the Covid19
pandemic, as part of a digital transformation initiative. These initial solutions
were high profile, highly personalized virtual assistants — like the Erica chatbot
from Bank of America. As the pandemic hit, the need changed as contact centers were
under increased pressures. As Cathal McGloin of ServisBOT explains in “how it started,
and how it is going,” financial institutions were looking for ways to automate
solutions to help get back to “normal” levels of customer service. This resulted
in a change from the “future of conversational AI” to a real tactical assistant
that can help in customer service. Haritha Dev of Wells Fargo, saw a similar trend.
Banks were originally looking to conversational AI as part of digital transformation
to keep up with the times. However, with the pandemic, it has been more about
customer retention and customer satisfaction. In addition, new use cases came about
as a result of Covid-19 that accelerated adoption of conversational AI. As Vinita
Kumar of Deloitte points out, banks were dealing with an influx of calls about new
concerns, like questions around the Paycheck Protection Program (PPP) loans. This
resulted in an increase in volume, without enough agents to assist customers, and
tipped the scale to incorporate conversational AI. When choosing initial use cases
to support, financial institutions often start with high volume, low complexity
tasks. For example, password resets, checking account balances, or checking the
status of a transaction, as Vinita points out. From there, the use cases can evolve
as the banks get more mature in developing conversational AI, and as the customers
become more engaged with the solutions. Cathal indicates another good way for banks
to start is looking at use cases that are a pain point, and also do not require a
lot of IT support. Some financial institutions may have a multi-year technology
roadmap, which can make it harder to get a new service started. A simple chatbot
for document collection in an onboarding process can result in high engagement,
and a high return on investment. For example, Cathal has a banking customer that
implemented a chatbot to capture a driver’s license to be used in the verification
process of adding an additional user to an account — it has over 85% engagement
with high satisfaction. An interesting use case Haritha discovered involved
educating customers on financial matters. People feel more comfortable asking a
chatbot what might be considered a “dumb” question, as the chatbot is less judgmental.
Users can be more ambiguous with their questions as well, not knowing the right
words to use, as chatbot can help narrow things down.
"""

inputs = ["summarize: " + text]

inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors="pt")
output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)
decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]
predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]

print(predicted_title)
# Conversational AI: The Future of Customer Service